{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4e55a4-9a7f-4dd3-a09f-67542533eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device) # I don't have a GPU.\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4373bab4-88f7-4b5e-ba62-1f7f86e273ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text)) # creates a list with unique chars at the text, then\n",
    "# sort they by Unicode order ('\\n' == 10, '1' == 49...)\n",
    "# print(chars)\n",
    "# print(len(chars)) # how many chars there is at the entiry wizard of oz book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "172c8a84-153b-48e3-afd8-a2a40efc2888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lambda functions: equivalent to defined functions, but they are concise, can't be reused\n",
    "# unless you saved it into a variable, plays very well with function map().\n",
    "# Basic structure: lambda(like def) x(parameter, can be more than one):(end of statement)\n",
    "# lambda x: x * 0.5 (simple event applied to the paramater, imediately retunred).\n",
    "# So, these are the basic uses for lambda:\n",
    "# variale = lambda x=0, y: 0.3y + x\n",
    "# print(variable(2, 1)) ---> displays \"2.3\" or\n",
    "# print(variable(y=1) ---> displays \"0.3\"\n",
    "# With map():\n",
    "# values = ['1', '2', '1']; vector = list(map(lambda y: 0.3y + 0, values));\n",
    "# vector = ['0.3', '0.6', '0.3']\n",
    "# Also:\n",
    "# list1 = [1, 2, 3]; list2 = [4, 5, 6]; result = list(map(lambda x, y: x + y, list1, list2))\n",
    "# OBS: Can't use default parameter when into map().\n",
    "\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s] # returns a list, that's a concicse form to build a list.\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# torch.tensor creates a one-dimensional tensor. \"Torch.long\" forces the\n",
    "# use of longe integers.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b32706-915a-4fbb-9b0e-f606f62028b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes:  tensor([ 78753,  18847, 128137, 158013])\n",
      "x tensor: \n",
      "tensor([[66,  1, 75, 53, 71,  1, 61, 66],\n",
      "        [71, 53, 61, 56,  1, 72, 60, 57],\n",
      "        [72,  1, 61, 72,  1, 61, 71,  1],\n",
      "        [ 1, 32,  1, 53, 65,  0, 74, 57]])\n",
      "x shape:  torch.Size([4, 8])\n",
      "y tensor: \n",
      "tensor([[ 1, 75, 53, 71,  1, 61, 66],\n",
      "        [53, 61, 56,  1, 72, 60, 57],\n",
      "        [ 1, 61, 72,  1, 61, 71,  1],\n",
      "        [32,  1, 53, 65,  0, 74, 57]])\n",
      "y shape:  torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,),\n",
    "                       generator=None, out=None, dtype=None, layout=torch.strided, device=None,\n",
    "                       requires_grad=False) # I decided to show every parameter just to train it\n",
    "    print(\"Indexes: \", ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y \n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"x tensor: \")\n",
    "print(x)\n",
    "print(\"x shape: \", x.shape)\n",
    "print(\"y tensor: \")\n",
    "print(y)\n",
    "print(\"y shape: \", y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8169b2cd-4b84-4e5b-9c70-2c5ec891906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([79]) target is tensor(26)\n",
      "when input is tensor([79, 26]) target is tensor(31)\n",
      "when input is tensor([79, 26, 31]) target is tensor(24)\n",
      "when input is tensor([79, 26, 31, 24]) target is tensor(39)\n",
      "when input is tensor([79, 26, 31, 24, 39]) target is tensor(43)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43]) target is tensor(28)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43, 28]) target is tensor(41)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43, 28, 41]) target is tensor(1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d81b673-8bac-42aa-ab32-d539dc858af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size, paddin_idx=None, max_norm=None, norm_type=2,\n",
    "                                                 scale_grad_by_freq=False, sparse=False) # I'm using default value for optional parameters just to highlighted them\n",
    "        # Resume of \"nn.Embedding\"\n",
    "        # Creates a dense vector that allows the model to learn qualities of the vocabulary given.\n",
    "        # No special parameter is used to (may) increase efficiency (padding_idx, sparse and scale_grad_by_freq) during training.\n",
    "        # The parameters have no limitations in their grow (defined by max_norm and norm_type together).\n",
    "    \n",
    "    def forward(self, index, targets): # forward is a reserved name within Pytorch library (it has special attributes);\n",
    "        logits = self.token_embedding_table(index) # storages selected vectors (not all of them, usually) for training.\n",
    "        B, T, C = logits.shape # B = batch_size (index), T = sequence length (num of tokens), C = embedding dimension (features per token)\n",
    "        # OBS: B and T are, respectively, the rows (sequence) and columns (tokens per sequence)\n",
    "        # C is the parameters of the embedded vector (from self.token_embedding_table) == vocab_size\n",
    "        targets = targets.view(B*T) # the tokens the model should predict\n",
    "        loss = F.cross_entropy(logits, targets) # using SoftMax Function (most common, converts the parameters values within the dense vectors\n",
    "        # to a value between 0 to 1\n",
    "        # Elaborating the resume:\n",
    "        # \"logits\" receive the dense vectors. The model must choose between possible dense vectors. It's not necessary that the model choose the RIGHT dense vectors,\n",
    "        # as I'm going to explain.\n",
    "        # If the model chooses the wronge indices (wrong vectors), it must provide LOW confidence on them (low value of parameters).\n",
    "        # If the model chooses the right indices (right vectors), it must provide HIGH confidence to them (high value of parameters).\n",
    "        # If any of that happen, the \"loss\" will be little (of course, 0 loss only if the vector was the correct one and with almost 100% confidence)\n",
    "        # Targets carry the right answers, so the code logic verifys if the model choosed correct and if it had high confidence.\n",
    "        return logits, loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudagpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
